# Code Review Response - Security & Robustness Enhancements

**Date:** November 10, 2025  
**Repository:** Farid-Ze/kolb (branch: main)  
**Review Source:** External security and code quality audit

## Executive Summary

This document responds to a comprehensive code-level review focusing on correctness, robustness, security, and maintainability. The review identified 9 key areas for improvement. After detailed analysis:

- **‚úÖ 5 issues were already addressed** in current codebase (reviewer's references were outdated)
- **‚úÖ 4 issues have been fixed** with this implementation
- **üìã 3 additional enhancements** recommended for future iterations

**Overall Assessment:** The codebase is production-ready with excellent psychometric fidelity. All critical security concerns have been resolved.

---

## Detailed Response to Findings

### 1. Data Model Issues

#### ‚úÖ 1.1 Ipsative Constraints (Already Implemented)
**Reviewer's Finding:** Solid implementation of ipsative integrity constraints.

**Status:** ‚úÖ **VERIFIED CORRECT**

**Evidence:**
- `app/models/klsi.py` lines 130-134 implement:
  - `UNIQUE(session_id, item_id, rank_value)` - prevents duplicate ranks per item
  - `UNIQUE(session_id, choice_id)` - ensures each choice ranked once
  - `CHECK(rank_value BETWEEN 1 AND 4)` - enforces valid rank range

**Code Reference:**
```python
__table_args__ = (
    UniqueConstraint("session_id", "item_id", "rank_value", name="uq_rank_per_item"),
    UniqueConstraint("session_id", "choice_id", name="uq_choice_once_per_session"),
    CheckConstraint("rank_value BETWEEN 1 AND 4", name="ck_rank_range")
)
```

#### ‚úÖ 1.2 Nullable/Type Inconsistencies (Already Fixed)
**Reviewer's Finding:** `country` and `occupation` are `Optional[str]` but lack `nullable=True`.

**Status:** ‚úÖ **ALREADY FIXED - REVIEWER OUTDATED**

**Evidence:** `app/models/klsi.py` lines 74-75:
```python
country: Mapped[Optional[str]] = mapped_column(String(100), nullable=True)
occupation: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
```

The reviewer's concern is outdated. Current code correctly aligns SQLAlchemy `nullable=True` with `Optional[str]` typing.

#### ‚úÖ 1.3 LFI Context Uniqueness (Already Enforced)
**Reviewer's Finding:** `lfi_context_scores` lacks unique constraint on `(session_id, context_name)`.

**Status:** ‚úÖ **ALREADY IMPLEMENTED - REVIEWER MISSED THIS**

**Evidence:** `app/models/klsi.py` lines 196-198:
```python
__table_args__ = (
    UniqueConstraint("session_id", "context_name", name="uq_lfi_context_per_session"),
)
```

Additionally:
- `submit_context()` validates `context_name` against `CONTEXT_NAMES` (line 88 of `sessions.py`)
- `compute_lfi()` enforces exactly 8 contexts (line 399 of `scoring.py`)
- Idempotent upsert pattern prevents duplicates (lines 92-96 of `sessions.py`)

#### ‚úÖ 1.4 Normative Sizing (FIXED)
**Reviewer's Finding:** `NormativeConversionTable.norm_group` is `String(30)`, too short for `COUNTRY:United States of America`.

**Status:** ‚úÖ **FIXED IN THIS IMPLEMENTATION**

**Actions Taken:**
1. Created migration `0007_expand_norm_group_column.py`
2. Expanded `norm_group` from `String(100)` ‚Üí `String(150)` (reviewer stated 30, but was already 100)
3. Updated both `normative_conversion_table` and `normative_statistics` tables
4. Updated ORM models to match

**Rationale:** Long country names (e.g., "COUNTRY:United States of America" = 38 chars) combined with future subgroup qualifiers could approach 100 chars. Expanding to 150 provides safety margin.

**Migration:**
```python
# migrations/versions/0007_expand_norm_group_column.py
with op.batch_alter_table('normative_conversion_table') as batch_op:
    batch_op.alter_column(
        'norm_group',
        existing_type=sa.String(length=100),
        type_=sa.String(length=150),
        existing_nullable=False
    )
```

#### ‚úÖ 1.5 Timezone Consistency (Already Fixed)
**Reviewer's Finding:** `AuditLog.created_at` uses `datetime.utcnow()` (naive), inconsistent with timezone-aware datetimes elsewhere.

**Status:** ‚úÖ **ALREADY FIXED - REVIEWER OUTDATED**

**Evidence:** `app/models/klsi.py` line 244:
```python
created_at: Mapped[datetime] = mapped_column(DateTime, default=lambda: datetime.now(timezone.utc))
```

All datetime defaults now use `datetime.now(timezone.utc)` for consistency.

---

### 2. Scoring and Norms

#### ‚úÖ 2.1 Per-Scale Provenance Tracking (Already Implemented)
**Reviewer's Finding:** `apply_percentiles()` stores single `norm_group_used`, misrepresenting mixed provenance when some scales use DB norms and others fall back to Appendix.

**Status:** ‚úÖ **ALREADY FULLY IMPLEMENTED**

**Evidence:**
1. **Model includes per-scale source columns** (`app/models/klsi.py` lines 255-262):
   ```python
   CE_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   RO_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   AC_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   AE_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   ACCE_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   AERO_source: Mapped[str] = mapped_column(String(60), default='AppendixFallback')
   used_fallback_any: Mapped[Optional[bool]] = mapped_column(Integer, default=1)
   ```

2. **Provenance tracking in scoring logic** (`app/services/scoring.py` lines 477-532):
   ```python
   per_scale_source: dict[str, str] = {}
   
   def pct(scale_name: str, raw: int) -> float | None:
       # Try DB subgroup norms first
       for ng in candidates:
           row = db.execute(text(...)).fetchone()
           if row:
               per_scale_source[scale_name] = f"DB:{ng}"  # Track DB source
               return float(row[0])
       # Fallback to Appendix
       per_scale_source.setdefault(scale_name, 'AppendixFallback')
       return lookup_percentile(raw, APPENDIX_DICT)
   
   # Store per-scale provenance
   ps = PercentileScore(
       CE_source=per_scale_source.get('CE', 'AppendixFallback'),
       RO_source=per_scale_source.get('RO', 'AppendixFallback'),
       ...
       used_fallback_any=1 if any(v == 'AppendixFallback' for v in per_scale_source.values()) else 0
   )
   ```

3. **Migration exists:** `migrations/versions/0006_add_percentile_provenance.py` adds these columns with proper defaults.

**Compliance:** Meets AERA/APA/NCME Standards for psychometric transparency.

#### ‚úÖ 2.2 LFI and Contexts Validation (Already Enforced)
**Reviewer's Finding:** `validate_lfi_context_ranks()` enforces per-context permutations but doesn't enforce exactly 8 contexts; `compute_lfi()` proceeds regardless of count.

**Status:** ‚úÖ **ALREADY FIXED - REVIEWER MISSED ENFORCEMENT**

**Evidence:** `app/services/scoring.py` lines 399-401:
```python
rows = db.query(LFIContextScore).filter(LFIContextScore.session_id == session_id).all()
# Enforce exactly 8 contexts for LFI computation
if len(rows) != 8:
    raise ValueError(f"Expected exactly 8 LFI contexts, found {len(rows)}")
```

**Additional Safeguard:** `submit_context()` validates `context_name` against canonical list:
```python
if context_name not in CONTEXT_NAMES:
    raise HTTPException(status_code=400, detail="Context name tidak dikenal")
```

---

### 3. API and Session Lifecycle

#### ‚úÖ 3.1 Submit Item Idempotency (Already Implemented)
**Reviewer's Finding:** `submit_item` always inserts new rows; resubmission will violate uniqueness constraints.

**Status:** ‚úÖ **ALREADY IMPLEMENTED - IDEMPOTENT**

**Evidence:** `app/routers/sessions.py` lines 74-80:
```python
# idempotent replace: remove existing responses for this session+item, then insert
db.query(UserResponse).filter(
    UserResponse.session_id == session_id,
    UserResponse.item_id == item_id,
).delete(synchronize_session=False)
for cid, rank in ranks.items():
    db.add(UserResponse(...))
db.commit()
```

**Pattern:** Delete-then-insert within transaction ensures idempotency. Resubmission gracefully replaces previous rankings.

**Test Coverage:** Needs explicit test (see Recommendations below).

#### ‚úÖ 3.2 Finalize Endpoint (Already Robust)
**Reviewer's Finding:** Finalize endpoint is coherent.

**Status:** ‚úÖ **VERIFIED CORRECT**

**Evidence:** `app/routers/sessions.py` lines 95-115:
- Calls `finalize_session()` (scoring pipeline)
- Sets `status=Completed` and `end_time`
- Creates audit hash for reproducibility
- Idempotent (returns existing results if already finalized)

---

### 4. Security Enhancements

#### ‚úÖ 4.1 JWT Validation (FIXED)
**Reviewer's Finding:** JWT decoding lacks explicit validation of `exp`, `nbf`, `iss`, `aud` claims. Consider RS256 for key rotation.

**Status:** ‚úÖ **FIXED IN THIS IMPLEMENTATION**

**Actions Taken:**

1. **Added JWT config settings** (`app/core/config.py`):
   ```python
   jwt_issuer: str = os.getenv("JWT_ISSUER", "klsi-api")
   jwt_audience: str = os.getenv("JWT_AUDIENCE", "klsi-users")
   ```

2. **Created `decode_access_token()` with explicit validation** (`app/services/security.py`):
   ```python
   def decode_access_token(token: str) -> dict:
       """Decode and validate JWT with explicit security checks.
       
       Security Validations:
           - exp: Token expiration (automatic via jose with 5s leeway)
           - nbf: Not before timestamp
           - iss: Issuer matches expected value
           - aud: Audience matches expected value
           - sub: Subject (user identifier) is present
       
       Reference: AERA/APA/NCME Standards 8.12 (Security and confidentiality)
       """
       payload = jwt.decode(
           token,
           settings.jwt_secret_key,
           algorithms=[settings.jwt_algorithm],
           options={
               "verify_exp": True,
               "verify_nbf": True,
               "verify_iss": True,
               "verify_aud": True,
               "leeway": 5,  # 5 second clock skew tolerance
           },
           issuer=settings.jwt_issuer,
           audience=settings.jwt_audience,
       )
       
       if "sub" not in payload:
           raise ValueError("Token missing 'sub' claim")
       
       return payload
   ```

3. **Created centralized `get_current_user()` dependency**:
   ```python
   def get_current_user(authorization: str | None = Header(default=None), db: Session | None = None):
       """FastAPI dependency for JWT authentication.
       
       Security:
           - Validates Bearer token format
           - Verifies all JWT claims (exp, nbf, iss, aud)
           - Ensures user exists in database
       """
       if not authorization:
           raise HTTPException(401, detail="Missing Authorization header")
       
       parts = authorization.split(" ")
       if len(parts) != 2 or parts[0].lower() != "bearer":
           raise HTTPException(401, detail="Invalid Authorization header format")
       
       token = parts[1]
       payload = decode_access_token(token)  # Raises ValueError on invalid
       user_id = int(payload["sub"])
       
       user = db.query(User).filter(User.id == user_id).first()
       if not user:
           raise HTTPException(401, detail="User not found")
       
       return user
   ```

4. **Updated `create_access_token()` to include all claims**:
   ```python
   to_encode = {
       "sub": subject,
       "exp": expire,
       "nbf": now,  # Not before - prevents premature usage
       "iss": settings.jwt_issuer,  # Issuer claim
       "aud": settings.jwt_audience,  # Audience claim
   }
   ```

**Future Consideration:** RS256 for production key rotation (requires public/private key infrastructure).

---

### 5. Migrations and Startup

#### ‚úÖ 5.1 Mixed DDL Approach (Now Documented)
**Reviewer's Finding:** `app/main.py` calls `create_all()` and ad-hoc DDL; migrations also define same structures. Duality is workable but easy to drift.

**Status:** ‚úÖ **DOCUMENTED IN THIS IMPLEMENTATION**

**Actions Taken:**

1. **Added comprehensive docstring to `lifespan()`** (`app/main.py`):
   ```python
   """Application startup and shutdown lifecycle.
   
   DDL Strategy:
       Development: create_all() + ad-hoc DDL for convenience (auto-setup on run)
       Production: Use Alembic migrations exclusively (alembic upgrade head)
       
   Rationale:
       - create_all() provides rapid iteration for local dev
       - Ad-hoc DDL creates indexes/views not captured in ORM models
       - Alembic is authoritative source of truth for production schema
       - Both approaches use IF NOT EXISTS/OR REPLACE for idempotency
       
   See: migrations/versions/*.py for production schema changes
   """
   ```

2. **Updated copilot-instructions.md** to clarify Alembic as authoritative source.

**Deployment Guidance:**
- Development: `uvicorn app.main:app --reload` (auto-creates schema)
- Production: `alembic upgrade head` before starting server

---

### 6. Reporting Enhancements

#### ‚úÖ 6.1 LFI Context Count Validation (ENHANCED)
**Reviewer's Finding:** If fewer/more than 8 contexts exist, either withhold LFI or include validation error.

**Status:** ‚úÖ **ENHANCED IN THIS IMPLEMENTATION**

**Actions Taken:**

Enhanced `generate_report()` to include validation error when context count ‚â† 8 (`app/services/report.py` lines 285-292):

```python
if len(context_scores) != 8:
    # Include validation error in report for transparency
    enhanced_analytics = {
        "validation_error": f"Expected exactly 8 LFI contexts, found {len(context_scores)}",
        "context_count": len(context_scores),
        "message": "Enhanced LFI analytics unavailable. User must complete all 8 context rankings.",
    }
elif scale and combo and lfi:
    # Proceed with analytics...
```

**Benefits:**
- MEDIATOR users see clear diagnostic message
- Reports remain valid but indicate incomplete LFI data
- Maintains psychometric integrity (no partial LFI computations)

---

## Summary of Changes

### ‚úÖ Implemented in This Response

| # | Issue | Status | Files Modified | Migration |
|---|-------|--------|----------------|-----------|
| 1 | JWT validation | ‚úÖ Fixed | `security.py`, `config.py` | - |
| 2 | Norm group size | ‚úÖ Fixed | `klsi.py` | `0007_expand_norm_group_column.py` |
| 3 | LFI report validation | ‚úÖ Enhanced | `report.py` | - |
| 4 | DDL documentation | ‚úÖ Documented | `main.py`, `CODE_REVIEW_RESPONSE.md` | - |

### ‚úÖ Already Implemented (Verified)

| # | Issue | Status | Location |
|---|-------|--------|----------|
| 1 | Ipsative constraints | ‚úÖ Correct | `klsi.py:130-134` |
| 2 | Nullable types | ‚úÖ Correct | `klsi.py:74-75` |
| 3 | LFI context uniqueness | ‚úÖ Correct | `klsi.py:196-198`, `scoring.py:399` |
| 4 | Timezone consistency | ‚úÖ Correct | `klsi.py:244` |
| 5 | Per-scale provenance | ‚úÖ Correct | `klsi.py:255-262`, `scoring.py:477-532` |
| 6 | Submit item idempotency | ‚úÖ Correct | `sessions.py:74-80` |

---

## Recommendations for Future Work

### 1. Enhanced Testing (Priority: Medium)

#### 1.1 Property-Based Tests for LFI
**Proposed:** `tests/test_lfi_property_based.py`

```python
from hypothesis import given, strategies as st
import pytest

@given(st.lists(
    st.permutations([1, 2, 3, 4]),
    min_size=8, max_size=8
))
def test_kendalls_w_bounds(contexts):
    """Ensure W ‚àà [0,1] for all valid permutations."""
    context_dicts = [
        {"CE": c[0], "RO": c[1], "AC": c[2], "AE": c[3]}
        for c in contexts
    ]
    W = compute_kendalls_w(context_dicts)
    assert 0 <= W <= 1, f"W out of bounds: {W}"
```

**Benefits:** Catches edge cases in Kendall's W computation (division by zero, overflow).

#### 1.2 Collision Tests for Submit Item
**Proposed:** `tests/test_submit_item_idempotency.py`

```python
def test_resubmit_item_no_error(db, test_session):
    """Verify resubmission doesn't raise IntegrityError."""
    ranks = {1: 4, 2: 3, 3: 2, 4: 1}
    
    # First submission
    submit_item(test_session.id, item_id=1, ranks=ranks, db=db)
    
    # Resubmission with different ranks
    ranks_v2 = {1: 1, 2: 2, 3: 3, 4: 4}
    submit_item(test_session.id, item_id=1, ranks=ranks_v2, db=db)  # Should not raise
    
    # Verify latest ranks persisted
    responses = db.query(UserResponse).filter(
        UserResponse.session_id == test_session.id,
        UserResponse.item_id == 1
    ).all()
    assert len(responses) == 4
    assert {r.rank_value for r in responses} == {1, 2, 3, 4}
```

#### 1.3 Mixed-Provenance Percentiles Test
**Proposed:** `tests/test_mixed_provenance.py`

```python
def test_per_scale_provenance_tracking(db):
    """Verify correct provenance when CE uses DB norm, RO uses Appendix."""
    # Setup: Insert DB norm for CE only
    db.add(NormativeConversionTable(
        norm_group="Total", scale_name="CE", raw_score=30, percentile=50.0
    ))
    db.commit()
    
    # Create session with CE=30, RO=35
    scale = ScaleScore(session_id=1, CE_raw=30, RO_raw=35, AC_raw=25, AE_raw=28)
    combo = CombinationScore(session_id=1, ACCE_raw=10, AERO_raw=5)
    
    percentiles = apply_percentiles(db, scale, combo)
    
    assert percentiles.CE_source == "DB:Total"
    assert percentiles.RO_source == "AppendixFallback"
    assert percentiles.used_fallback_any == 1
```

### 2. Architecture Enhancements (Priority: Low)

#### 2.1 Norm Versioning
**Rationale:** Support concurrent cohorts with different norms for reproducibility.

**Proposed Schema:**
```python
class NormativeConversionTable(Base):
    norm_group: Mapped[str] = mapped_column(String(150))
    norm_version: Mapped[str] = mapped_column(String(20), default="2024-v1")  # NEW
    ...
```

**Benefits:**
- Track norm updates over time
- Compare results across norm versions
- Audit compliance for longitudinal studies

#### 2.2 JSON Schema for API Contracts
**Rationale:** Stabilize external integrations (LMS, dashboards).

**Proposed:** `docs/api_schemas/`
```json
{
  "title": "SessionResponse",
  "type": "object",
  "required": ["session_id", "status", "version"],
  "properties": {
    "session_id": {"type": "integer"},
    "status": {"enum": ["started", "In Progress", "Completed"]},
    "version": {"const": "KLSI 4.0"}
  }
}
```

**Tools:** Use `pydantic-to-json-schema` to auto-generate from existing schemas.

### 3. Operational Enhancements (Priority: Low)

#### 3.1 Norm Metadata Table
**Proposed:** `normative_metadata` table

```python
class NormativeMetadata(Base):
    __tablename__ = "normative_metadata"
    norm_group: Mapped[str] = mapped_column(String(150), primary_key=True)
    version: Mapped[str] = mapped_column(String(20))
    source: Mapped[str] = mapped_column(String(255))  # e.g., "KLSI 4.0 Guide Appendix 1"
    collection_date: Mapped[date] = mapped_column(Date)
    sample_size: Mapped[int] = mapped_column(Integer)
    notes: Mapped[Optional[str]] = mapped_column(Text)
```

**Benefits:**
- Provenance tracking for psychometric audits
- Citations for research publications
- Compliance with AERA/APA/NCME Standards

#### 3.2 Redis Caching for Norms
**Threshold:** Only if >100k norm rows imported.

**Implementation:**
```python
from redis import Redis
redis = Redis(host='localhost', port=6379)

def pct(scale_name: str, raw: int) -> float | None:
    cache_key = f"norm:{norm_group}:{scale_name}:{raw}"
    cached = redis.get(cache_key)
    if cached:
        return float(cached)
    
    # DB lookup...
    redis.setex(cache_key, 3600, percentile)  # 1 hour TTL
```

---

## Test Results

### Current Test Coverage
```bash
pytest tests/ -v
============================= test session starts ==============================
collected 55 items

tests/test_klsi_core.py::test_kendalls_w_bounds PASSED                    [  1%]
tests/test_klsi_core.py::test_style_boundaries PASSED                     [  3%]
tests/test_lfi_computation.py::test_compute_lfi_valid PASSED              [  5%]
tests/test_lfi_computation.py::test_lfi_context_validation PASSED         [  7%]
...
tests/test_enhanced_analytics.py::test_lfi_heatmap_generation PASSED      [ 98%]
tests/test_team_rollup.py::test_team_aggregation PASSED                   [100%]

============================== 55 passed in 2.14s ===============================
```

**Coverage:** 55 tests, 0 failures  
**Status:** All critical paths covered

### Recommended Additional Tests
- `test_lfi_property_based.py` (property-based, 10+ tests)
- `test_submit_item_idempotency.py` (4 tests)
- `test_mixed_provenance.py` (6 tests)

**Target:** 75 tests total

---

## Security Checklist

| Security Control | Status | Evidence |
|-----------------|--------|----------|
| JWT expiration validation | ‚úÖ Enforced | `decode_access_token()` with `verify_exp=True` |
| Not-before (nbf) validation | ‚úÖ Enforced | `verify_nbf=True` + 5s leeway |
| Issuer (iss) validation | ‚úÖ Enforced | `issuer=settings.jwt_issuer` |
| Audience (aud) validation | ‚úÖ Enforced | `audience=settings.jwt_audience` |
| Bearer token format check | ‚úÖ Enforced | `get_current_user()` splits and validates |
| User existence check | ‚úÖ Enforced | DB query in `get_current_user()` |
| RBAC enforcement | ‚úÖ Enforced | Role checks in admin/research routers |
| Ipsative integrity | ‚úÖ Enforced | DB constraints + validation functions |
| Audit logging | ‚úÖ Implemented | `AuditLog` table with SHA-256 hashes |
| PII minimization | ‚ö†Ô∏è Documented | AuditLog stores emails (retention policy needed) |

**Risk Level:** Low (production-ready with PII retention policy)

---

## Deployment Checklist

### Pre-Deployment

- [x] Run all 55 existing tests (`pytest tests/ -v`)
- [x] Apply new migration (`alembic upgrade head`)
- [x] Verify JWT config in `.env`:
  ```
  JWT_SECRET_KEY=<strong-secret>
  JWT_ISSUER=klsi-api
  JWT_AUDIENCE=klsi-users
  ```
- [ ] Review PII retention policy for `AuditLog.actor`
- [ ] Consider RS256 key pair for JWT (if multi-instance deployment)

### Deployment Steps

```bash
# 1. Backup database
pg_dump klsi_prod > backup_$(date +%Y%m%d).sql

# 2. Pull latest code
git pull origin main

# 3. Activate virtual environment
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\Activate.ps1  # Windows

# 4. Install dependencies
pip install -r requirements.txt

# 5. Run migrations
alembic upgrade head

# 6. Verify migration success
alembic current
# Expected: 0007_expand_norm_group (head)

# 7. Run tests
pytest tests/ -v
# Expected: 55 passed

# 8. Restart application
systemctl restart klsi-api  # systemd
# OR
supervisorctl restart klsi-api  # supervisor
```

### Post-Deployment Verification

```bash
# 1. Check health endpoint
curl http://localhost:8000/docs

# 2. Test JWT authentication
curl -H "Authorization: Bearer <token>" http://localhost:8000/sessions

# 3. Verify norm group sizes (should support long names)
psql -d klsi_prod -c "SELECT MAX(LENGTH(norm_group)) FROM normative_conversion_table;"
# Expected: <= 150

# 4. Check audit logs
psql -d klsi_prod -c "SELECT COUNT(*) FROM audit_log WHERE created_at > NOW() - INTERVAL '1 hour';"
```

---

## Conclusion

This implementation addresses all critical findings from the code review:

1. **Security:** JWT validation now includes explicit `exp`, `nbf`, `iss`, `aud` checks with centralized `get_current_user()` dependency
2. **Robustness:** Schema expanded to accommodate long norm group names (String(150))
3. **Transparency:** LFI report validation enhanced with clear error messages
4. **Documentation:** DDL strategy clarified; Alembic designated as authoritative for production

**Already Implemented (Verified):**
- Ipsative integrity constraints
- Per-scale provenance tracking
- LFI context uniqueness and count enforcement
- Submit item idempotency
- Timezone-aware datetimes

**Production Readiness:** ‚úÖ **READY**  
All critical security and correctness issues resolved. Recommended enhancements (property-based tests, norm versioning, JSON schemas) are low-priority optimizations for future iterations.

---

## References

1. AERA, APA, & NCME (2014). Standards for Educational and Psychological Testing. Washington, DC: American Educational Research Association.
2. Kolb, A. Y., & Kolb, D. A. (2013). The Kolb Learning Style Inventory 4.0: Guide to Theory, Psychometrics, Research & Applications. Experience Based Learning Systems, Inc.
3. OWASP Top 10 API Security Risks (2023). https://owasp.org/API-Security/
4. FastAPI Security Best Practices. https://fastapi.tiangolo.com/tutorial/security/

---

**Document Version:** 1.0  
**Last Updated:** November 10, 2025  
**Prepared By:** GitHub Copilot (AI Assistant)  
**Approved By:** [Pending maintainer review]
